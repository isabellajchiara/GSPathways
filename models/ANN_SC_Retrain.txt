library(AlphaSimR)
library(readxl)
library(writexl)
library(rrBLUP)
library(keras)
library(tensorflow)
library(readr)
library(BMTME)

rm(TrainingPheno)
rm(TrainingGeno)
rm(y)
rm(M)

## create GRM ##
geno <- as.matrix(TrainingGeno)
GM <- tcrossprod(geno)/dim(geno)
LG <- cholesky(GM)

Y <- as.matrix(TrainingPheno)
X = LG

## create data frame with geno and pheno data ##

phenotypes <- as.data.frame(Y)
genotypes <- as.data.frame(geno)
data <-cbind(phenotypes,genotypes)

## stratified clustering training testing partition##

y <- as.data.frame(TrainingPheno)
M <- as.data.frame(TrainingGeno)

newgeno <- M %>%  select(where(~ n_distinct(.) > 1))

colnames(newgeno) =NULL

PCAgeno <- prcomp(newgeno, center=TRUE, scale=TRUE) ##take out categorical columns##

PCAselected = as.data.frame(-PCAgeno$x[,1:3])

silhouette <- fviz_nbclust(PCAselected, kmeans, method = 'silhouette')
kvalues <- silhouette$data ##largest value tells how many clusters are optimal ##
kvalues <- kvalues[order(-kvalues$y),]

k=as.numeric(kvalues[1,1])

kmeans_geno = kmeans(PCAselected, centers = k, nstart = 50)
clusters <- fviz_cluster(kmeans_geno, data = PCAselected)

clusterData <- clusters$data

clusterData <- clusterData[order(clusterData$cluster),]

nclusters <- as.numeric(clusterData[as.numeric(nrow(clusterData)),as.numeric(ncol(clusterData))])

i = 1
datalist = vector("list", length = nclusters)
for (i in 1:nclusters) {
  clustername <- paste0("cluster",i)
  clustername <- clusterData[clusterData$cluster==i,] 
  
  assign(paste0("cluster",i), clustername)
  
  trnname <- paste0("trn",i)
  trnname <- clustername[sample(0.3*nrow(clustername)),]
  datalist[[i]] <- trnname

  i = i + 1
  if (i > nclusters){ 
    break
  }
  }
  
TRN <- do.call(rbind, datalist)
TRN <- na.omit(TRN)

TRN <- TRN[,1]

OptimGeno <- M[TRN,]
y <- as.data.frame(y)
OptimPheno <- y[TRN,]

BV <- OptimPheno

trainingset= as.data.frame(cbind(BV,OptimGeno))
colnames(trainingset) <- paste("ID",1:(ncol(y) + ncol(M)), sep="")


## one holdout CV ##

No_Epoch = 1000
N_Units = 33
X_trn = M[TRN,]
X_tst = M[-TRN,]
Y_trn = Y[TRN,]
Y_tst = Y[-TRN,]

## Build model using pipe operator ##

build_model <- function() {
  model <- keras_model_sequential()
  model %>%
    layer_dense(units =N_Units, activation = "relu", input_shape = c(dim
                                                                     (X_trn)[2])) %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units = 1, activation = "linear")
  model %>% compile(
    loss = "mse",
    optimizer = "rmsprop",
    metrics = c("mse"))
  model}

## build and view model ##

model <- build_model()
model %>% summary()

## fitting and plotting the model ##

print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 20 == 0) cat("\n")
    cat(".")
  })

## inner training and validation ##

model_fit <- model %>% fit(
  X_trn, Y_trn,
  shuffle=F,
  epochs = No_Epoch, batch_size = 640,
  validation_split = 0.2,
  verbose = 0,
  callbacks = list(print_dot_callback))

## plot ## 

plot(model_fit)

## Refit model using early stopping ##

early_stop <- callback_early_stopping(monitor="val_loss", mode='min', patience =50)

model_Final2 <- build_model()
model_fit_Final <- model_Final%>%fit(
  X_trn, Y_trn,
  shuffle=F,
  epochs = No_Epoch, batch_size=640,
  validation_split = 0.2,
  verbose=0, callbacks = list(early_stop, print_dot_callback)
)

## plot history of training process ##

length(model_fit_Final$metrics$mean_squared_error)
plot(model_fit_Final)

## predition to see how model performs##

prediction = model_Final2 %>% predict(X_tst)
Predicted = c(prediction)
Observed = Y_tst
plot(Observed, Predicted)
MSE = mean((Observed-Predicted)^2)
MSE
Obs_Pred = cbind(Observed, Predicted)
colnames(Obs_Pred) = c("Observed", "Predicted")
Obs_Pred

###### Model is built, continue with breeding simulation #####
